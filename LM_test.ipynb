{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Keyring is skipped due to an exception: Failed to unlock the collection!\u001b[0m\n",
      "\u001b[33mWARNING: Keyring is skipped due to an exception: Failed to unlock the collection!\u001b[0m\n",
      "\u001b[33mWARNING: Keyring is skipped due to an exception: Failed to unlock the collection!\u001b[0m\n",
      "\u001b[33mWARNING: Keyring is skipped due to an exception: Failed to unlock the collection!\u001b[0m\n",
      "Collecting pytorch-pretrained-bert\n",
      "\u001b[33m  WARNING: Keyring is skipped due to an exception: Failed to unlock the collection!\u001b[0m\n",
      "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in ./anaconda3/lib/python3.8/site-packages (from pytorch-pretrained-bert) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in ./anaconda3/lib/python3.8/site-packages (from pytorch-pretrained-bert) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in ./anaconda3/lib/python3.8/site-packages (from pytorch-pretrained-bert) (1.16.39)\n",
      "Requirement already satisfied, skipping upgrade: regex in ./anaconda3/lib/python3.8/site-packages (from pytorch-pretrained-bert) (2020.6.8)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in ./anaconda3/lib/python3.8/site-packages (from pytorch-pretrained-bert) (4.54.1)\n",
      "Requirement already satisfied, skipping upgrade: torch>=0.4.1 in ./anaconda3/lib/python3.8/site-packages (from pytorch-pretrained-bert) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in ./anaconda3/lib/python3.8/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in ./anaconda3/lib/python3.8/site-packages (from requests->pytorch-pretrained-bert) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in ./anaconda3/lib/python3.8/site-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./anaconda3/lib/python3.8/site-packages (from requests->pytorch-pretrained-bert) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in ./anaconda3/lib/python3.8/site-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in ./anaconda3/lib/python3.8/site-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.20.0,>=1.19.39 in ./anaconda3/lib/python3.8/site-packages (from boto3->pytorch-pretrained-bert) (1.19.39)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in ./anaconda3/lib/python3.8/site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in ./anaconda3/lib/python3.8/site-packages (from botocore<1.20.0,>=1.19.39->boto3->pytorch-pretrained-bert) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in ./anaconda3/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.39->boto3->pytorch-pretrained-bert) (1.15.0)\n",
      "\u001b[31mERROR: allennlp 0.8.5 has requirement torch<1.2,>=0.4.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pytorch-pretrained-bert\n",
      "  Attempting uninstall: pytorch-pretrained-bert\n",
      "    Found existing installation: pytorch-pretrained-bert 0.6.1\n",
      "    Uninstalling pytorch-pretrained-bert-0.6.1:\n",
      "      Successfully uninstalled pytorch-pretrained-bert-0.6.1\n",
      "Successfully installed pytorch-pretrained-bert-0.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/kim/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/kim/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file /home/kim/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp1wegb228\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3842\n",
      "['nation']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text = '[CLS] Germany is a [MASK] . [SEP]'\n",
    "masked_index = tokenized_text.index('[MASK]')\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Create the segments tensors.\n",
    "segments_ids = [0] * len(tokenized_text)\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "# Predict all tokens\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(tokens_tensor, segments_tensors)\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
    "print(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import string\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased').eval()\n",
    "\n",
    "from transformers import XLNetTokenizer, XLNetLMHeadModel\n",
    "xlnet_tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "xlnet_model = XLNetLMHeadModel.from_pretrained('xlnet-base-cased').eval()\n",
    "\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM\n",
    "xlmroberta_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "xlmroberta_model = XLMRobertaForMaskedLM.from_pretrained('xlm-roberta-base').eval()\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large').eval()\n",
    "\n",
    "from transformers import ElectraTokenizer, ElectraForMaskedLM\n",
    "electra_tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-generator')\n",
    "electra_model = ElectraForMaskedLM.from_pretrained('google/electra-small-generator').eval()\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = RobertaForMaskedLM.from_pretrained('roberta-base').eval()\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "\n",
    "def decode(tokenizer, pred_idx, top_clean):\n",
    "    ignore_tokens = string.punctuation + '[PAD]'\n",
    "    tokens = []\n",
    "    for w in pred_idx:\n",
    "        token = ''.join(tokenizer.decode(w).split())\n",
    "        if token not in ignore_tokens:\n",
    "            tokens.append(token.replace('##', ''))\n",
    "    return ' '.join(tokens[:top_clean])\n",
    "\n",
    "\n",
    "def encode(tokenizer, text_sentence, add_special_tokens=True):\n",
    "    text_sentence = text_sentence.replace('<mask>', tokenizer.mask_token)\n",
    "    # if <mask> is the last token, append a \".\" so that models dont predict punctuation.\n",
    "    if tokenizer.mask_token == text_sentence.split()[-1]:\n",
    "        text_sentence += ' .'\n",
    "\n",
    "    input_ids = torch.tensor([tokenizer.encode(text_sentence, add_special_tokens=add_special_tokens)])\n",
    "    mask_idx = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n",
    "    return input_ids, mask_idx\n",
    "\n",
    "\n",
    "def get_all_predictions(text_sentence, top_clean=5):\n",
    "    # ========================= BERT =================================\n",
    "    input_ids, mask_idx = encode(bert_tokenizer, text_sentence)\n",
    "    with torch.no_grad():\n",
    "        predict = bert_model(input_ids)[0]\n",
    "    bert = decode(bert_tokenizer, predict[0, mask_idx, :].topk(top_k).indices.tolist(), top_clean)\n",
    "    print(bert)\n",
    "    # ========================= XLNET LARGE =================================\n",
    "    input_ids, mask_idx = encode(xlnet_tokenizer, text_sentence, False)\n",
    "    perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
    "    perm_mask[:, :, mask_idx] = 1.0  # Previous tokens don't see last token\n",
    "    target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float)  # Shape [1, 1, seq_length] => let's predict one token\n",
    "    target_mapping[0, 0, mask_idx] = 1.0  # Our first (and only) prediction will be the last token of the sequence (the masked token)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predict = xlnet_model(input_ids, perm_mask=perm_mask, target_mapping=target_mapping)[0]\n",
    "    xlnet = decode(xlnet_tokenizer, predict[0, 0, :].topk(top_k).indices.tolist(), top_clean)\n",
    "\n",
    "    # ========================= XLM ROBERTA BASE =================================\n",
    "    input_ids, mask_idx = encode(xlmroberta_tokenizer, text_sentence, add_special_tokens=True)\n",
    "    with torch.no_grad():\n",
    "        predict = xlmroberta_model(input_ids)[0]\n",
    "    xlm = decode(xlmroberta_tokenizer, predict[0, mask_idx, :].topk(top_k).indices.tolist(), top_clean)\n",
    "\n",
    "    # ========================= BART =================================\n",
    "    input_ids, mask_idx = encode(bart_tokenizer, text_sentence, add_special_tokens=True)\n",
    "    with torch.no_grad():\n",
    "        predict = bart_model(input_ids)[0]\n",
    "    bart = decode(bart_tokenizer, predict[0, mask_idx, :].topk(top_k).indices.tolist(), top_clean)\n",
    "\n",
    "    # ========================= ELECTRA =================================\n",
    "    input_ids, mask_idx = encode(electra_tokenizer, text_sentence, add_special_tokens=True)\n",
    "    with torch.no_grad():\n",
    "        predict = electra_model(input_ids)[0]\n",
    "    electra = decode(electra_tokenizer, predict[0, mask_idx, :].topk(top_k).indices.tolist(), top_clean)\n",
    "\n",
    "    # ========================= ROBERTA =================================\n",
    "    input_ids, mask_idx = encode(roberta_tokenizer, text_sentence, add_special_tokens=True)\n",
    "    with torch.no_grad():\n",
    "        predict = roberta_model(input_ids)[0]\n",
    "    roberta = decode(roberta_tokenizer, predict[0, mask_idx, :].topk(top_k).indices.tolist(), top_clean)\n",
    "\n",
    "    return {'bert': bert,\n",
    "            'xlnet': xlnet,\n",
    "            'xlm': xlm,\n",
    "            'bart': bart,\n",
    "            'electra': electra,\n",
    "            'roberta': roberta}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Germany is a <mask>.\n",
      "nation\n",
      "country\n",
      "state\n",
      "republic\n",
      "language\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bert': 'nation\\ncountry\\nstate\\nrepublic\\nlanguage',\n",
       " 'xlnet': 'a\\nsub\\nvery\\nda\\na',\n",
       " 'xlm': 'country\\nnation\\nworld\\nEuropean\\ngreat',\n",
       " 'bart': 'German\\ncountry\\nstate\\nsurname\\nrepublic',\n",
       " 'electra': 'country\\nnation\\nman\\ncity\\ngerman',\n",
       " 'roberta': 'democracy\\ncountry\\nnation\\nmess\\nrepublic'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_predictions('Germany is a <mask>.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
