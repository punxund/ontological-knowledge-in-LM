# Exploring Ontological Knowledge in Language Models

Ontological knowledge that can be used in a knowledge graph or in the SemanticWeb are
manually created and stored by human. If ontological knowledge can be automatically
extracted using a language model that has shown improvements in several knowledgeintensive
fields, it will be possible to increase the eciency of knowledge expansion.
Therefore, we evaluate the ontological knowledge about two properties â€™instance of â€™ and
â€™subclass of â€™, that is encoded in pre-trained language models. We conducted extensive
experiments comparing the triples predicted by the pre-trained language model (BERT)
and the extracted triples from a knowledge graph (Wikidata). We show that the language
model is able to predict the ontological knowledge of the items we often use. The predictions
for some categories, e.g., â€™countryâ€™ and â€™car manufactererâ€™ work very well. But overall,
there is still room for improvement. If the language model is more specialized and trained
to extract ontological knowledge, it will be possible to expand the knowledge graph as well
as create the knowledge base through this technology.
